# WarpDrive Forge

A Go training workload that demonstrates [WarpDrive](https://github.com/vpatelsj/WarpDrive) serving globally replicated datasets to ML-style jobs via FUSE. The binary reads COCO-style WebDataset TAR shards from WarpDrive-mounted POSIX paths — even shards from Canada Central, odd shards from West US 3 — exercising cross-region data access with zero Azure SDK calls. All storage semantics are delegated to WarpDrive.

## Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│  Azure VM  (Ubuntu 24.04, Standard_D4s_v5)                      │
│                                                                  │
│  warpdrive-forge ──▸ /wd/datasets-cac/train/shard-000000.tar    │
│       (training)     /wd/datasets-wus3/train/shard-000001.tar    │
│            │                    ▲                                 │
│            │                    │  FUSE                           │
│            │         ┌──────────┴──────────┐                     │
│            │         │  warpdrive-mount    │                     │
│            │         │  :9090 /metrics     │                     │
│            │         └────┬──────────┬─────┘                     │
│            │              │          │                            │
└────────────┼──────────────┼──────────┼───────────────────────────┘
             │              │          │
             │    ┌─────────▼─┐  ┌─────▼──────────┐
             │    │ Azure Blob│  │  Azure Blob    │
             │    │ Canada    │  │  West US 3     │
             │    │ Central   │  │                │
             │    │ (even)    │  │  (odd shards)  │
             │    └───────────┘  └────────────────┘
             │
             ▼
     stdout: step=100 images/s=920.4 loss=1.83
```

## Repo Layout

```
cmd/warpdrive-forge/     CLI entry point with signal handling
internal/
  config/                Strict YAML loader + CLI overrides
  dataset/               Shard discovery, TAR pairing, deterministic sampler
  model/                 Simple softmax classifier (CPU-only)
  trainer/               Training loop with batching, preprocessing, metrics
  metrics/               Sliding-window throughput & latency stats
configs/demo.yaml        Default training config
demo/                    Ready-to-run scripts (cold, warm, metrics tailing)
infra/                   Azure provisioning & deployment scripts (00–06)
```

## Prerequisites

| Component | Version | Notes |
|-----------|---------|-------|
| Azure CLI | >= 2.50 | `brew install azure-cli` (macOS) |
| Go | >= 1.22 (forge), >= 1.24 (WarpDrive) | |
| OS (VM) | Ubuntu 24.04 LTS | Installed by provisioning script |
| FUSE | fuse3 / libfuse3-dev | Installed by setup script |
| Python 3 | >= 3.8 | For synthetic dataset generation |
| Azure subscription | — | Permissions for RG, storage, VM, RBAC |

## Quick Start — End-to-End Deployment

> Full step-by-step guide with architecture details: [DEPLOY.md](DEPLOY.md)

### From your laptop

```bash
az login
source infra/00-env.sh          # set shared variables

./infra/01-provision.sh          # create Azure RG, storage, VM, RBAC
./infra/02-gen-dataset.sh        # generate & upload 20 WebDataset shards

# Copy repo to the VM
scp -r . azureuser@$VM_IP:~/warpdrive-forge
ssh azureuser@$VM_IP
```

### On the VM

```bash
# Set storage account names (from infra/.env.generated on your laptop)
export STORAGE_ACCOUNT_A="<canadacentral account name>"
export STORAGE_ACCOUNT_B="<westus3 account name>"

./infra/03-vm-setup.sh                # install Go 1.24, FUSE, build everything
./infra/04-gen-warpdrive-config.sh    # write /etc/warpdrive/config.yaml
./infra/05-run.sh                     # mount → warm cache → train → metrics snapshot
```

### Teardown

```bash
./infra/06-teardown.sh               # deletes the entire resource group
```

## WarpDrive Config — Key Settings

The config generated by `04-gen-warpdrive-config.sh` uses rclone's azureblob provider under the hood. These settings are **required** for correct operation:

| Key | Value | Why |
|-----|-------|-----|
| `root` | `coco2017-wds` | WarpDrive maps this to rclone's `remotePath` (not `container`) |
| `chunk_size` | `"4194304"` | rclone azureblob requires non-zero chunk size |
| `list_chunk` | `"5000"` | Prevents `maxresults=0` query parameter error |
| `env_auth` | `"true"` | Let rclone handle Azure auth directly |
| `use_msi` | `"true"` | Enable managed identity authentication |

## Usage

```bash
# Build
go build -o bin/warpdrive-forge ./cmd/warpdrive-forge

# Run (with WarpDrive already mounted at /wd)
bin/warpdrive-forge \
  -config configs/demo.yaml \
  -train-root-a /wd/datasets-cac/train \
  -train-root-b /wd/datasets-wus3/train \
  -steps 200 -batch-size 16 -num-workers 4 -seed 42
```

### CLI Flags

| Flag | Default | Description |
|------|---------|-------------|
| `-config` | `configs/demo.yaml` | Path to YAML config |
| `-train-root-a` | from config | POSIX path to even-shard root |
| `-train-root-b` | from config | POSIX path to odd-shard root |
| `-steps` | 2000 | Number of training steps |
| `-batch-size` | 64 | Batch size |
| `-num-workers` | 8 | Data loader worker goroutines |
| `-seed` | 42 | PRNG seed for reproducibility |
| `-log-every` | 100 | Print metrics every N steps |

## WarpDrive Metrics

WarpDrive exposes Prometheus metrics at `:9090/metrics`. Key counters:

| Metric | Description |
|--------|-------------|
| `warpdrive_fuse_operations_total` | FUSE ops by type (read, lookup, readdir) |
| `warpdrive_fuse_operation_duration_seconds` | FUSE latency histogram |
| `warpdrive_cache_hit_total` / `cache_miss_total` | Cache hit/miss counts |
| `warpdrive_backend_bytes_read_total` | Bytes read per backend |
| `warpdrive_backend_request_duration_seconds` | Backend latency (list, stat, read) |
| `warpdrive_auth_refresh_total` | Credential refresh events |

Monitor during training:

```bash
watch -n2 'curl -s http://localhost:9090/metrics | grep "^warpdrive_"'
# Or use the included script:
./demo/03_watch_warpdrive_metrics.sh
```

## Observed Performance (20-shard demo)

From a live training run on `Standard_D4s_v5` (200 steps, batch 16, 4 workers):

| Metric | Value |
|--------|-------|
| Total FUSE reads | **1,001,187** |
| Mean FUSE read latency | **154 μs** |
| P99.5 FUSE read latency | < 500 μs |
| Cache hit rate | **100%** |
| Backend data read (CAC) | 24.0 GB |
| Backend data read (WUS3) | 24.0 GB |
| Backend I/O errors | **0** |
| Auth method | Managed Identity (Entra) |

## Infra Scripts Reference

| Script | Where | Description |
|--------|-------|-------------|
| `00-env.sh` | Laptop | Shared environment variables (customizable) |
| `01-provision.sh` | Laptop | Create Azure RG, storage accounts, VM, RBAC |
| `02-gen-dataset.sh` | Laptop | Generate synthetic WebDataset shards & upload |
| `03-vm-setup.sh` | VM | Install Go, FUSE, build WarpDrive + forge into `bin/` |
| `04-gen-warpdrive-config.sh` | VM | Write WarpDrive YAML config with rclone-compatible keys |
| `05-run.sh` | VM | Clean stale mounts, mount, warm cache, train, print metrics |
| `06-teardown.sh` | Laptop | Delete all Azure resources |

## Design Notes

- **Zero cloud SDK calls** — all reads go through POSIX mounts (`/wd/...`). Swap in any FUSE-compatible filesystem and the training code works unchanged.
- **Deterministic sampling** — the multi-root sampler interleaves shards across regions with a seeded PRNG, ensuring reproducible training regardless of cloud topology.
- **GPU-ready** — replace `internal/model/simplecnn.go` with a GPU-backed implementation and the data pipeline stays intact. The POSIX interface means no plumbing changes for DGX migrations.
